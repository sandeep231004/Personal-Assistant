# Context Tracking Test Guide

## Overview
This document contains test scenarios to verify that the agent maintains context about files, directories, documents, and notes mentioned throughout a conversation session.

---

## ğŸ¯ What Was Improved

### **Enhanced System Prompt with:**
1. **Explicit Contextual Reference Tracking** - Agent now actively looks back at conversation history
2. **Concrete Examples** - Shows agent exactly how to handle pronouns and vague references
3. **System Message Awareness** - Agent checks system messages for uploads, note creation, file operations
4. **Clarification Instructions** - Agent will ask for clarification instead of guessing

### **Key Features:**
- Tracks "it", "that", "this", "those" references
- Remembers last mentioned documents, notes, files, folders
- Cross-references system messages in conversation history
- Maintains context across multi-turn conversations

---

## ğŸ§ª Test Scenarios

### **Scenario 1: Document Upload Context**

**Test Flow:**
```
1. User: "Upload this AI research paper"
   â†’ Upload document: "AI_Research_2025.pdf"
   â†’ System adds message: "[SYSTEM] Document uploaded: 'AI_Research_2025.pdf'"

2. User: "What does it say about transformers?"
   â†’ Expected: Agent recognizes "it" = "AI_Research_2025.pdf"
   â†’ Agent uses rag_search on that specific document
   â†’ Response: Information about transformers from the paper

3. User: "Summarize that document"
   â†’ Expected: Agent recognizes "that document" = "AI_Research_2025.pdf"
   â†’ Agent provides summary of the same document
```

**What to Verify:**
- âœ… Agent doesn't ask "which document?"
- âœ… Agent correctly identifies the referenced document
- âœ… Agent uses rag_search tool (not web search)

---

### **Scenario 2: Note Context Tracking**

**Test Flow:**
```
1. User: "Save a note titled 'Meeting Notes' with content: Discussed Q1 goals"
   â†’ System saves note
   â†’ System adds message: "[SYSTEM] Note saved: 'Meeting Notes'"

2. User: "What's in it?"
   â†’ Expected: Agent recognizes "it" = "Meeting Notes"
   â†’ Agent uses retrieve_note for "Meeting Notes"
   â†’ Response: Shows the content

3. User: "Add 'Review budget' to that note"
   â†’ Expected: Agent recognizes "that note" = "Meeting Notes"
   â†’ Agent uses edit_note with mode='append'
   â†’ Response: Confirms addition to Meeting Notes
```

**What to Verify:**
- âœ… Agent maintains awareness of which note was just created
- âœ… Agent doesn't ask "which note?"
- âœ… Agent correctly appends to the right note

---

### **Scenario 3: Multiple Files - Disambiguation**

**Test Flow:**
```
1. User: "I uploaded two files: ML_Guide.pdf and Python_Tutorial.pdf"
   â†’ Upload both documents
   â†’ System adds messages for both uploads

2. User: "What does it say about neural networks?"
   â†’ Expected: Agent asks for clarification OR searches both documents
   â†’ Better response: "I see you uploaded two files. Which one should I search?"

3. User: "The ML guide"
   â†’ Expected: Agent now searches ML_Guide.pdf specifically
   â†’ Response: Information from ML_Guide.pdf only
```

**What to Verify:**
- âœ… Agent recognizes ambiguity when multiple files exist
- âœ… Agent asks for clarification instead of guessing
- âœ… After clarification, agent remembers the specific file

---

### **Scenario 4: Cross-Reference System Messages**

**Test Flow:**
```
1. User: "Save a note called 'Project Ideas'"
   â†’ System message: "[SYSTEM] Note saved: 'Project Ideas'"

2. [Several unrelated queries about weather, time, etc.]

3. User: "Update the note I created earlier"
   â†’ Expected: Agent checks system messages in history
   â†’ Agent finds "[SYSTEM] Note saved: 'Project Ideas'"
   â†’ Agent uses list_notes â†’ retrieve_note â†’ asks what to add
   â†’ Response: Shows current content and asks for new content
```

**What to Verify:**
- âœ… Agent looks back through entire conversation history
- âœ… Agent identifies notes from system messages
- âœ… Agent maintains context despite intervening queries

---

### **Scenario 5: File Operation Context**

**Test Flow:**
```
1. User: "List files in the documents folder"
   â†’ Agent uses execute_command or appropriate tool
   â†’ Shows: file1.txt, file2.txt, file3.txt

2. User: "What's in that folder?"
   â†’ Expected: Agent recognizes "that folder" = "documents folder"
   â†’ Agent lists the same folder again (or confirms previous answer)

3. User: "Check the second file"
   â†’ Expected: Agent recognizes "second file" = "file2.txt" from previous listing
   â†’ Agent attempts to read file2.txt
```

**What to Verify:**
- âœ… Agent remembers which folder was just listed
- âœ… Agent tracks file positions from previous responses
- âœ… Agent correctly identifies numbered references

---

### **Scenario 6: Pronoun Chaining**

**Test Flow:**
```
1. User: "I saved a note about AI automation"
   â†’ System saves note

2. User: "Can you read it back to me?"
   â†’ Expected: Agent retrieves the note about "AI automation"

3. User: "Thanks. Now add 'Use Python' to it"
   â†’ Expected: Agent appends to the SAME note (maintains chain)

4. User: "Perfect. Show me that note again"
   â†’ Expected: Agent shows the SAME note with the addition
```

**What to Verify:**
- âœ… Agent maintains reference across multiple turns
- âœ… Pronouns chain correctly (it â†’ it â†’ that note all reference same object)
- âœ… Context persists throughout conversation

---

### **Scenario 7: Contextual Switching**

**Test Flow:**
```
1. User: "Upload resume.pdf"
   â†’ Document uploaded

2. User: "What's the weather?"
   â†’ Agent provides weather (unrelated query)

3. User: "What does my resume say about experience?"
   â†’ Expected: Agent remembers "resume.pdf" despite intervening query
   â†’ Agent searches resume.pdf using rag_search
```

**What to Verify:**
- âœ… Agent maintains document context despite topic changes
- âœ… Agent returns to correct context when user references it again
- âœ… Unrelated queries don't erase previous context

---

## ğŸ“Š Success Metrics

### **Pass Criteria:**
âœ… Agent correctly identifies references 90%+ of the time
âœ… Agent asks for clarification when ambiguous
âœ… Agent maintains context across 5+ conversation turns
âœ… Agent checks system messages for context clues
âœ… No false positives (agent doesn't apply old context to new queries)

### **Failure Indicators:**
âŒ Agent asks "which file?" when only one file exists
âŒ Agent applies wrong context (references wrong file/note)
âŒ Agent ignores previous context and performs web search instead of RAG
âŒ Agent loses context after 2-3 turns
âŒ Agent applies old context to unrelated new queries

---

## ğŸ”§ How to Test

### **Option 1: API Testing (FastAPI /docs)**
```bash
# Start server
cd backend
uvicorn app.main:app --reload

# Visit http://localhost:8000/docs
# Use /api/chat endpoint with same session_id for all messages
```

**Example Request:**
```json
{
  "message": "Upload complete. What does the document say?",
  "session_id": "test_session_123"
}
```

### **Option 2: Python Test Script**
```python
import requests

API_URL = "http://localhost:8000/api/chat"
SESSION_ID = "context_test_001"

def send_message(message):
    response = requests.post(API_URL, json={
        "message": message,
        "session_id": SESSION_ID
    })
    return response.json()

# Test Scenario 1
print(send_message("I just saved a note called 'Ideas'"))
print(send_message("What's in it?"))  # Should recognize "it" = "Ideas" note
print(send_message("Add 'Build mobile app' to that note"))  # Should edit Ideas note
```

---

## ğŸ› Debugging Context Issues

### **If Agent Loses Context:**

1. **Check Conversation History:**
   ```
   GET /api/conversations/{session_id}
   ```
   Verify system messages are being saved

2. **Check Logs:**
   ```bash
   # Look for:
   "[Session: {id}] Loaded {N} previous messages"
   ```

3. **Verify System Messages:**
   - Document uploads should create system messages
   - Note creation should create system messages
   - Check database: `SELECT * FROM conversations WHERE role='system'`

4. **Test Conversation History Loading:**
   - Ensure last 5 messages are loaded correctly
   - Check message order (chronological)

---

## ğŸ“ Implementation Details

### **What Changed:**
**File:** `backend/app/agents/voice_agent.py` (lines 219-268)

**Added:**
- Explicit "CONTEXTUAL REFERENCE TRACKING" section in system prompt
- Concrete examples of pronoun â†’ object mapping
- Instructions to check system messages
- Clarification instructions for ambiguity

**Key Instructions to Agent:**
```
- ALWAYS maintain awareness of files, folders, documents, and notes mentioned
- When user uses pronouns, LOOK BACK at the conversation
- Track what was discussed: 'the document' = last mentioned document
- Check system messages in history for uploads, note creation
- If unclear, ask for clarification instead of guessing
```

---

## ğŸ¯ Expected Behavior After Enhancement

### **Before:**
```
User: "I uploaded a research paper"
[uploads paper.pdf]
User: "What does it say?"
Agent: "I'm not sure which document you're referring to. Could you upload it?"
âŒ Agent lost context
```

### **After:**
```
User: "I uploaded a research paper"
[uploads paper.pdf â†’ System message added]
User: "What does it say?"
Agent: [Checks history, sees system message about paper.pdf]
Agent: [Uses rag_search on paper.pdf]
Agent: "Based on the research paper you just uploaded, it discusses..."
âœ… Agent maintained context
```

---

## ğŸš€ Next Steps

1. **Run all test scenarios** above
2. **Document any failures** and edge cases
3. **Consider adding:**
   - More specific system messages (e.g., file paths, note IDs)
   - Entity tracking in agent state
   - Explicit reference resolution step before tool calling
4. **Monitor** conversation logs for context loss patterns

---

**Status:** âœ… **Enhanced - Ready for Testing**
**Last Updated:** 2025-10-09
